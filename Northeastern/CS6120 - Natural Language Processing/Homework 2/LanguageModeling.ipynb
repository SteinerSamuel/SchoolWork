{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LanguageModeling.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dasmiq/CS6120-HW2/blob/master/LanguageModeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moWB9udaKesP"
      },
      "source": [
        "Your task is to train *character-level* language models. \n",
        "You will train unigram, bigram, and trigram character-level models on a collection of books from Project Gutenberg. You will then use these trained English language models to distinguish English documents from Brazilian Portuguese documents in the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHFJmuftHJld",
        "outputId": "7879ca79-e483-400a-f860-4fd46fc59291",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "from nltk.util import ngrams, flatten\n",
        "from nltk import FreqDist\n",
        "from sklearn.model_selection import train_test_split\n",
        "from string import punctuation\n",
        "import httpimport\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "with httpimport.remote_repo(['lm_helper'], 'https://raw.githubusercontent.com/jasoriya/CS6120-PS2-support/master/utils/'):\n",
        "  from lm_helper import get_train_data, get_test_data"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\Sam\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package gutenberg to\n[nltk_data]     C:\\Users\\Sam\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package gutenberg is already up-to-date!\n[nltk_data] Downloading package brown to\n[nltk_data]     C:\\Users\\Sam\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package brown is already up-to-date!\n[nltk_data] Downloading package mac_morpho to\n[nltk_data]     C:\\Users\\Sam\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package mac_morpho is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8U0UCuyHQkai"
      },
      "source": [
        "This code loads the training and test data. Each dataset is a list of books. Each book contains a list of sentences, and each sentence contains a list of words. For building a character language model, you should join the words of a sentence together with a space character."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6x0pfuiEChTh"
      },
      "source": [
        "# get the train and test data\n",
        "train = get_train_data()\n",
        "test, test_files = get_test_data()\n",
        "\n",
        "# split the training data \n",
        "train, train_left_out = train_test_split(train, test_size=.2)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "source": [
        "## 1.1\n",
        "Collect statistics on the unigram, bigram, and trigram character counts.\n",
        "\n",
        "If your machine takes a long time to perform this computation, you may save these counts to files in your github repository and load them on request. This is not necessary, however."
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "_WAO9VjFLArq"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oh4VOoiSIoUF",
        "tags": []
      },
      "source": [
        "uniCount = FreqDist()\n",
        "biCount = FreqDist()\n",
        "triCount = FreqDist()\n",
        "uniGrams = []\n",
        "biGrams = []\n",
        "triGrams = []\n",
        "\n",
        "for book in train:\n",
        "    for sentence in book:\n",
        "        s = ' '.join(sentence).lower()\n",
        "        # # get rid of numbers\n",
        "        # s = re.sub(\"\\d+\", \"\", s).lower()\n",
        "        # # remove punctuation\n",
        "        # s = s.translate(str.maketrans('', '', punctuation))\n",
        "        # if s and s[-1] == ' ':\n",
        "        #     s = s[:-1] # gets rid of any trailling spaces\n",
        "        uni = ngrams(s, 1)\n",
        "        bi = ngrams(s, 2)\n",
        "        tri = ngrams(s, 3) \n",
        "\n",
        "        # update counters\n",
        "        un = [u for u in uni]\n",
        "        bn = [b for b in bi]\n",
        "        tn = [t for t in tri]\n",
        "        uniCount.update([''.join(u) for u in un])\n",
        "        biCount.update([''.join(b) for b in bn])\n",
        "        triCount.update([''.join(t) for t in tn])\n",
        "        uniGrams.append(un)\n",
        "        biGrams.append(bn)\n",
        "        triGrams.append(tn)\n",
        "\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(' ', 2230425), ('e', 981459), ('t', 728992), ('a', 643403), ('o', 593513), ('h', 579387), ('n', 539058), ('i', 503494), ('s', 489093), ('r', 436650)]\n[('e ', 377235), (' t', 308978), ('th', 298761), ('he', 258362), ('d ', 236991), (' a', 211213), ('t ', 189643), ('s ', 189439), (' ,', 171789), (', ', 167231)]\n[(' th', 231760), ('the', 185521), (' , ', 167023), ('he ', 150205), ('nd ', 104299), ('and', 99411), (' an', 97037), (' of', 68307), ('of ', 65672), ('ed ', 57146)]\n"
          ]
        }
      ],
      "source": [
        "print(uniCount.most_common(10))\n",
        "print(biCount.most_common(10))\n",
        "print(triCount.most_common(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RS3mnaIvQnhI"
      },
      "source": [
        "## 1.2\n",
        "Calculate the perplexity for each document in the test set using the linear interpolation smoothing method. For determining λs for linear interpolation, you can divide the training data into a new training set (80%) and a held-out set (20%), then using grid search method:\n",
        "Choose ~10 values of λ to test using grid search on held-out data.\n",
        "\n",
        "Some documents in the test set are in Brazilian Portuguese. Identify them as follows: \n",
        "  - Sort by perplexity and set a cut-off threshold. All the documents above this threshold score should be categorized as Brazilian Portuguese. \n",
        "  - Print the file names (from `test_files`) and perplexities of the documents above the threshold\n",
        "\n",
        "    ```\n",
        "        file name, score\n",
        "        file name, score\n",
        "        . . .\n",
        "        file name, score\n",
        "    ```\n",
        "\n",
        "  - Copy this list of filenames and manually annotate them as being correctly or incorrectly labeled as Portuguese.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQF4HhQGOZD8",
        "tags": []
      },
      "source": [
        "def perplexity(lambda_1, lambda_2, lambda_3, tri_count, bi_count, uni_count, document):\n",
        "    tokenized_document = []\n",
        "    for sentence in document:\n",
        "        s = ' '.join(sentence).lower()\n",
        "        tri = ngrams(s, 3)\n",
        "        tokenized_document.append([''.join(token) for token in tri])\n",
        "\n",
        "    tokenized_document = flatten(tokenized_document)\n",
        "    probs = []\n",
        "    for token in tokenized_document: # calc the probability\n",
        "        p3 = (lambda_3\n",
        "         * (tri_count[token]\n",
        "         /bi_count.get(token[:-1], 1))) + (lambda_2 * (\n",
        "             bi_count[token[:-1]]\n",
        "             /uni_count.get(token[:-2], 1))) + (lambda_3 * \n",
        "             (uni_count[token[:-2]]\n",
        "             /sum(uni_count.values())))\n",
        "        \n",
        "        if p3 == 0:\n",
        "            p3 = 1/sum(uni_count.values())\n",
        "        \n",
        "        probs.append(p3)\n",
        "    \n",
        "    l_probs = sum(np.log10(probs))\n",
        "    return 2**(-l_probs/len(tokenized_document))\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "lambdas = [\n",
        "    [.35, .24, .41],\n",
        "    [.225, .125, .750],\n",
        "    [.56, .24, .1],\n",
        "    [.3, .5, .2],\n",
        "    [.35, .20, .45],\n",
        "    [.40, .40, .2],\n",
        "    [.12, .24, .64],\n",
        "    [.123, .522, .355],\n",
        "    [.66, .155, .185],\n",
        "    [.250, .60, .15]\n",
        "]\n",
        "\n",
        "best_lambda = {}\n",
        "for ls in lambdas:\n",
        "    perlex_list = []\n",
        "    for document in train_left_out:\n",
        "        perlex_list.append(perplexity(ls[0], ls[1], ls[2], triCount, biCount, uniCount, document))\n",
        "    \n",
        "    best_lambda[sum(perlex_list)/len(perlex_list)] = ls\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "lambdas_to_use = best_lambda[min(best_lambda.keys())]\n",
        "\n",
        "brazillian_docs = []\n",
        "\n",
        "for document, file_name in zip(test, test_files):\n",
        "    perplex = perplexity(lambdas_to_use[0], lambdas_to_use[1], lambdas_to_use[2], triCount, biCount, uniCount, document)\n",
        "    threshold = 1.75\n",
        "    if perplex > threshold:\n",
        "        brazillian_docs.append([file_name, perplex, document])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ag94ma03.txt', 2.131517746123798]\n['br94jl01.txt', 2.102406366439263]\n['ag94ou04.txt', 2.106702704371739]\n['ag94mr1.txt', 2.102899858413237]\n['br94fe1.txt', 2.1309273260011725]\n['br94ju01.txt', 2.101198034130712]\n['br94ma01.txt', 2.098401522494839]\n['br94ab02.txt', 2.0930243421247345]\n['ag94ag02.txt', 2.1089128763593705]\n['ag94ab12.txt', 2.1240380191712336]\n['br94ag01.txt', 2.113743064919677]\n['ag94no01.txt', 2.108940515898756]\n['ag94fe1.txt', 2.080846596592233]\n['ag94se06.txt', 2.12907451126396]\n['ag94de06.txt', 2.105830886340917]\n['ag94ju07.txt', 2.125475699045007]\n['ag94jl12.txt', 2.1159451021069584]\n['br94de01.txt', 2.102674427746029]\n['ag94ja11.txt', 2.094127710230229]\n['br94ja04.txt', 2.114897223736423]\n"
          ]
        }
      ],
      "source": [
        "for d  in [[_[0], _[1]] for _ in brazillian_docs]:\n",
        "    print(d)\n",
        "\n"
      ]
    },
    {
      "source": [
        "'ag94ab12.txt', correct\n",
        "\n",
        "'ag94mr1.txt', correct\n",
        "\n",
        "'br94ma01.txt', correct\n",
        "\n",
        "'ag94se06.txt', correct\n",
        "\n",
        "'ag94ja11.txt', correct\n",
        "\n",
        "'ag94ju07.txt', correct\n",
        "\n",
        "'br94fe1.txt', correct\n",
        "\n",
        "'ag94ag02.txt', correct\n",
        "\n",
        "'ag94de06.txt', correct\n",
        "\n",
        "'ag94ma03.txt', correct\n",
        "\n",
        "'ag94fe1.txt', correct\n",
        "\n",
        "'ag94jl12.txt', correct\n",
        "\n",
        "'ag94no01.txt', correct\n",
        "\n",
        "'br94ab02.txt', correct\n",
        "\n",
        "'br94de01.txt', correct\n",
        "\n",
        "'br94ag01.txt', correct\n",
        "\n",
        "'ag94ou04.txt', correct\n",
        "\n",
        "'br94jl01.txt', correct\n",
        "\n",
        "'br94ja04.txt', correct\n",
        "\n",
        "'br94ju01.txt', correct"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQl2u_giVW5e"
      },
      "source": [
        "\n",
        "# 1.3\n",
        "Build a trigram language model with add-λ smoothing (use λ = 0.1).\n",
        "\n",
        "Sort the test documents by perplexity and perform a check for Brazilian Portuguese documents as above:\n",
        "\n",
        "  - Observe the perplexity scores and set a cut-off threshold. All the documents above this threshold score should be categorized as Brazilian Portuguese. \n",
        "  - Print the file names and perplexities of the documents above the threshold\n",
        "\n",
        "  ```\n",
        "      file name, score\n",
        "      file name, score\n",
        "      . . .\n",
        "      file name, score\n",
        "  ```\n",
        "\n",
        "  - Copy this list of filenames and manually annotate them for correctness."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGUTEk8QUehL"
      },
      "source": [
        "def perplexity_add_lambda(lambda_v, tri_count, bi_count, uni_count, document):\n",
        "    tokenized_document = []\n",
        "    for sentence in document:\n",
        "        s = ' '.join(sentence).lower()\n",
        "        tri = ngrams(s, 3)\n",
        "        tokenized_document.append([''.join(token) for token in tri])\n",
        "\n",
        "    tokenized_document = flatten(tokenized_document)\n",
        "    probs = []\n",
        "    for token in tokenized_document: # calc the probability\n",
        "        p3 = (tri_count[token]/bi_count.get(token[:-1], 1)) + lambda_v\n",
        "        \n",
        "        probs.append(p3)\n",
        "    \n",
        "    l_probs = sum(np.log10(probs))\n",
        "    return 2**(-l_probs/len(tokenized_document))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "brazillian_docs_add_lambda = []\n",
        "\n",
        "for document, file_name in zip(test, test_files):\n",
        "    perplex = perplexity_add_lambda(.1, triCount, biCount, uniCount, document)\n",
        "    threshold = 1.75\n",
        "    if perplex > threshold:\n",
        "        brazillian_docs_add_lambda.append([file_name, perplex, document])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "for d  in [[_[0], _[1]] for _ in brazillian_docs_add_lambda]:\n",
        "    print(d)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "brazillian_docs_add_lambda_low = []\n",
        "\n",
        "for document, file_name in zip(test, test_files):\n",
        "    perplex = perplexity_add_lambda(.1, triCount, biCount, uniCount, document)\n",
        "    threshold = 1.65\n",
        "    if perplex > threshold:\n",
        "        brazillian_docs_add_lambda_low.append([file_name, perplex, document])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['ag94ma03.txt', 1.667372849268755]\n['br94jl01.txt', 1.6604762968418614]\n['ag94ou04.txt', 1.6662110547901836]\n['ag94mr1.txt', 1.6629577287153132]\n['br94fe1.txt', 1.6631193298834606]\n['br94ju01.txt', 1.6611312547962334]\n['br94ma01.txt', 1.6663247480456658]\n['br94ab02.txt', 1.6617274730284959]\n['ag94ag02.txt', 1.6635803647612344]\n['ag94ab12.txt', 1.6638829627942946]\n['br94ag01.txt', 1.6626830130604813]\n['ag94no01.txt', 1.661853422442727]\n['ag94fe1.txt', 1.6588192089402212]\n['ag94se06.txt', 1.6663991325896095]\n['ag94de06.txt', 1.6620537942864082]\n['ag94ju07.txt', 1.6671296104540596]\n['ag94jl12.txt', 1.6628946488958636]\n['br94de01.txt', 1.6619672646456796]\n['ag94ja11.txt', 1.6639252414763903]\n['br94ja04.txt', 1.6601570368734788]\n"
          ]
        }
      ],
      "source": [
        "for d  in [[_[0], _[1]] for _ in brazillian_docs_add_lambda_low]:\n",
        "    print(d)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqhXTB5TXR25"
      },
      "source": [
        "## 1.4\n",
        "Based on your observation from above questions, compare linear interpolation and add-λ smoothing by listing out their pros and cons."
      ]
    },
    {
      "source": [
        "When comparing the two models made in this notebook it is important to see that with the threshold selected (1.75) the simple add k or add λ smoothing was not able to select any documents as being not-english. But taking a closer look at the data we can see that with a slight modification with a threshold of 1.65 we do get the documents back, but the problem is we still recieve documents which are english which have been classifed as non-english. we can move on to the pros and cons\n",
        "\n",
        "Linear Interpolation\n",
        "\n",
        "    Pros:\n",
        "        Robust, linear Interpolation gives a more realistic probability of a character appearing because of the backoff model. The model allows for less data to be used since we are augmenting our dataset with the \n",
        "        bigrams and unigramas. The model seemingly seperates the documents slightly better with the difference between the highest scoring english document and the lowest scoring non english document being rather far \n",
        "        apart in comparission to the add k model.\n",
        "\n",
        "    Cons:\n",
        "        Requires more computation, since we have to calculate the bigram, and unigrams as well as the trigrams the model requires more computation.\n",
        "\n",
        "Add K/λ smoothing\n",
        "    \n",
        "    Pros:\n",
        "        Simple, doesn't require much preprocessing since we just need the trigrams this allows for rapid development of the model.\n",
        "    \n",
        "    Cons:\n",
        "        the more robust you want the model you need a much larger dataset compred to the Linear Interpolation model.\n",
        "        \n"
      ],
      "cell_type": "markdown",
      "metadata": {
        "id": "tFq1ECgDI6QG"
      }
    }
  ]
}